# -*- coding: utf-8 -*-
"""Part-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FbDOApE-dIwbw0ITfb8iFE4gzs4KQSeM
"""

import pandas as pd
from itertools import combinations
# Function to generate frequent itemsets
def generate_frequent_itemsets(transactions, min_support):
    # Implementation of the brute force method
    item_counts = {}
    for transaction in transactions:
        for item in transaction:
            if item in item_counts:
                item_counts[item] += 1
            else:
                item_counts[item] = 1

    frequent_itemsets = []
    for item, count in item_counts.items():
        support = count / len(transactions)
        if support >= min_support:
            frequent_itemsets.append({item})

    k = 2
    while True:
        candidate_itemsets = set(combinations(item_counts.keys(), k))
        frequent_itemsets_k = []
        for candidate in candidate_itemsets:
            support = sum(1 for transaction in transactions if set(candidate).issubset(set(transaction))) / len(transactions)
            if support >= min_support:
                frequent_itemsets_k.append(set(candidate))
        if not frequent_itemsets_k:
            break
        frequent_itemsets.extend(frequent_itemsets_k)
        k += 1

    return frequent_itemsets

# Function to generate association rules
def generate_association_rules(frequent_itemsets, min_confidence):
    association_rules = []
    for itemset in frequent_itemsets:
        if len(itemset) >= 2:
            for i in range(1, len(itemset)):
                for antecedent in combinations(itemset, i):
                    antecedent = set(antecedent)
                    consequent = itemset - antecedent
                    antecedent_support = sum(1 for itemset in frequent_itemsets if antecedent.issubset(itemset)) / len(frequent_itemsets)
                    rule_confidence = len(itemset) / len(antecedent)
                    if rule_confidence >= min_confidence:
                        association_rules.append((antecedent, consequent, antecedent_support, rule_confidence))
    return association_rules

# Set minimum support and confidence
min_support = 0.2
min_confidence = 0.7

# Load transactions from CSV files for each dataset
amazon_transactions = pd.read_csv("amazon_transactions.csv")['Transaction'].apply(lambda x: set(x.split(", "))).tolist()
best_buy_transactions = pd.read_csv("best_buy_transactions.csv")['Transaction'].apply(lambda x: set(x.split(", "))).tolist()
k_mart_transactions = pd.read_csv("k_mart_transactions.csv")['Transaction'].apply(lambda x: set(x.split(", "))).tolist()
nike_transactions = pd.read_csv("nike_transactions.csv")['Transaction'].apply(lambda x: set(x.split(", "))).tolist()
flipkart_transactions = pd.read_csv("flipkart_transactions.csv")['Transaction'].apply(lambda x: set(x.split(", "))).tolist()

# Generate frequent itemsets and association rules for each dataset
datasets = {
    "Amazon": amazon_transactions,
    "Best Buy": best_buy_transactions,
    "K-mart": k_mart_transactions,
    "Nike": nike_transactions,
    "Flipkart": flipkart_transactions
}

for dataset_name, dataset_transactions in datasets.items():
    frequent_itemsets = generate_frequent_itemsets(dataset_transactions, min_support)
    association_rules = generate_association_rules(frequent_itemsets, min_confidence)
    print(f"\n{dataset_name} Frequent Itemsets:")
    for itemset in frequent_itemsets:
        print(itemset)
    print(f"\n{dataset_name} Association Rules:")
    for rule in association_rules:
        print(rule)